1.Explain the difference between FIFO and Capacity scheduler


FIFO Scheduler:
*It is simple and easy to understand and need not configure aything but its not suitable for shared clusters.
*The FIFO scheduler places the application in a queue and process them in the order of submission ie First-In-First-Out basis.
*Requests for the first application in the queue are allocated first and once the request is served, the next application in the 
queue is process and so on.
*Larger applications use all the resources in the cluster. So each applications has to wait for its turn.
*On shared cluster, it is recommended to use the capacity scheduler or fair scheduler.
*Fair scheduling is a method of assigning resources to jobs such that all jobs 
get, on average, an equal share of resources over time. When there is a single 
job running, that job uses the entire cluster. When other jobs are submitted,
 tasks slots that free up are assigned to the new jobs, so that each job gets 
roughly the same amount of CPU time. Unlike the default Hadoop scheduler, 
which forms a queue of jobs, this lets short jobs finish in reasonable time while 
not starving long jobs. It is also a reasonable way to share a cluster between
 a number of users. Finally, fair sharing can also work with job priorities - the priorities are used as weights to determine the fraction of total compute time that each job should get.

Capacity Scheduler:

*A separate dedicated queue allows the small job to start as soon as it is submitted.
*This is the cost of overall cluster utilization since the capacity of the queue is reserved for jobs in that queue.
*If the queues are not designed or used properly some queues may be overloaded while some may be underutilised.
*Large jobs finishes later when comapred with using FIFO scheduler.
*The CapacityScheduler is designed to allow sharing a large cluster while
 giving each organization a minimum capacity guarantee. The central idea is 
that the available resources in the Hadoop Map-Reduce cluster are partitioned among multiple organizations who collectively fund the cluster based on computing needs. 
There is an added benefit that an organization can access any excess capacity no being used by others. This provides elasticity for the organizations in a cost-effective manner.

2.What are the limitations of hadoop 1.x and how they were overcome in hadoop 2.x:

Limitations of hadoop 1.x:

Hadoop 1.x has many limitations. Main drawback of Hadoop 1.x is that MapReduce 
component which supports only MApReduce-based
Batcg/Data Processing Applications.

*It is not suitable for real time data processing and data streaming.
*It has a single component: JobTracker to perform many activities like resource management, job scheduling, job monitoring, etc.
*JobTracker is the single point of failure.
*It does not supports multi-tenancy support and supports only one NameNode  and one Namespace per cluster.
*It does not supports horizontal scalability.
*It runs only map/reduce jobs.

Hadoop 2.x:

*By decoupling MapReduce component responsibilities into different components.
*By introducing new YARN component for Resource Management.
*By decoupling component's responsibilities, it supports multiple namespace.
*High availability and high scalability.
*Multi-tenancy is supported.